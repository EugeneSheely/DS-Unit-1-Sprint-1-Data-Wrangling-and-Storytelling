{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yiptel Data Scrap 7/17/2020",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugeneSheely/DS-Unit-1-Sprint-1-Data-Wrangling-and-Storytelling/blob/master/Yiptel_Data_Scrap_7_17_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNrH-KnW9KAV",
        "colab_type": "text"
      },
      "source": [
        "# **MOST CURRENT VERSION: **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-tp_9UGbSdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\"\"Hello Master, it's your humble slave, please place the original URL from page one, don't forget to add filters, like \"Fences & Gates\" and I'll search it\"\"\")\n",
        "originalurl = str(input(\"\"\"Enter Yelp URL: \"\"\"))\n",
        "csv_name = str(input(\"Add the name for the exported CSV file: \"))+ \".csv\"\n",
        "\n",
        "print(\"\\nThanks for the instructions O mighty master, I'll get working to it now...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument(\"-headless\")\n",
        "options.add_argument(\"-no-sandbox\")\n",
        "options.add_argument(\"-disable-dev-shm-usage\")\n",
        "\n",
        "\n",
        "\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.select import Select\n",
        "import unittest\n",
        "import re, urllib.request, time\n",
        "from time import sleep\n",
        "import random\n",
        "from IPython.display import clear_output, display\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import requests \n",
        "\n",
        "  \n",
        "# import only html class \n",
        "from lxml import html\n",
        " \n",
        "# search_term = input(\"Hello Master, its your humble data scrapping slave. What shall I fetch for you? \")\n",
        "# search_city = input(\"What city is this in? \")\n",
        "# Search_state = input(\"What state is this in? \")\n",
        "\n",
        "#for DF: \n",
        "dflist = []\n",
        "rowlist = []\n",
        "columns = [\"Name\", \"Website\", \"Phone\", \"Address\"]\n",
        "search_facebook_results = []\n",
        "search_email_results = []\n",
        "is_hyperlink = []\n",
        "hyperlink_leftovers = []\n",
        "\n",
        "\n",
        "name = \"EMPTY\"\n",
        "domain = \"EMPTY\"\n",
        "phone = \"EMPTY\"\n",
        "mapaddress = \"EMPTY\"\n",
        "category = \"EMPTY\"\n",
        "to_df = []\n",
        "#df = pd.DataFrame([to_df], columns = [\"Name\", \"URL\", \"Facebook\", \"Phone\", \"Category\", \"Address\", \"Emails\"])\n",
        "\n",
        "\n",
        "urlabout = [ originalurl,\n",
        "             originalurl + \"&start=10\",\n",
        "             originalurl + \"&start=20\",\n",
        "             originalurl + \"&start=30\",\n",
        "             originalurl + \"&start=40\",\n",
        "             originalurl + \"&start=50\",\n",
        "             originalurl + \"&start=60\",\n",
        "             originalurl + \"&start=70\",\n",
        "             originalurl + \"&start=80\",\n",
        "             originalurl + \"&start=90\",\n",
        "             originalurl + \"&start=100\",\n",
        "             originalurl + \"&start=110\",\n",
        "             originalurl + \"&start=120\",\n",
        "             originalurl + \"&start=130\",\n",
        "             originalurl + \"&start=140\",\n",
        "             originalurl + \"&start=150\",\n",
        "             originalurl + \"&start=160\",\n",
        "             originalurl + \"&start=170\",\n",
        "             originalurl + \"&start=180\",\n",
        "             originalurl + \"&start=190\",\n",
        "            originalurl + \"&start=200\",\n",
        "            originalurl + \"&start=210\",\n",
        "            originalurl + \"&start=220\",\n",
        "            originalurl + \"&start=230\",\n",
        "            originalurl + \"&start=240\",\n",
        "            originalurl + \"&start=250\",\n",
        "]\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "n=0\n",
        "list = []\n",
        "\n",
        "#COLUMNS\n",
        "print(\"Name\", \" | Website\", \" | Phone\", \" | Company Category\",\" | Address\" , \" | Emails\" ,   )\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# xpaths for general titel page search\n",
        "name1 = \"//li[5]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name2 = \"//li[6]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name3 = \"//li[7]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name4 = \"//li[8]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name5 = \"//li[9]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name6 = \"//li[10]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name7 = \"//li[11]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name8 = \"//li[12]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name9 = \"//li[13]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name10 = \"//li[14]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name11 = \"//li[15]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "name12 = \"//li[24]/div[1]/div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/h4[1]/span[1]/a[1]\"\n",
        "\n",
        "\n",
        "#xpath for fencers site\n",
        "\n",
        "domainxpath = \"//div[1]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/p[2]\"\n",
        "phonexpath = \"//div[2]/div[1]/div[@class='lemon--div__373c0__1mboc arrange-unit__373c0__o3tjT arrange-unit-fill__373c0__3Sfw1 border-color--default__373c0__3-ifU' and 2]/p[2]\"\n",
        "categoryxpath = \"//span[@class='lemon--span__373c0__3997G text__373c0__2Kxyz text-color--black-extra-light__373c0__2OyzO text-align--left__373c0__2XGa- text-size--large__373c0__3t60B']/a[1]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#xpath for address\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "namesxpathlist = [name2,name3,name4,name5,name6,name7,name8,name9,name10,name11, name12]\n",
        "fencersitexpathlist = []\n",
        "namelist = []\n",
        "\n",
        "\n",
        "#LOOP STARTS\n",
        "\n",
        "\n",
        "for i in range(len(urlabout)):\n",
        "  #print(\"1 \",name)\n",
        "  \n",
        "  print(\"\\nCurrently Searching in: \",  urlabout[i],\"\\n\")\n",
        "\n",
        "  url = webdriver.Chrome(\"chromedriver\",options=options)\n",
        "  url.get(urlabout[i])\n",
        "  url = r\"{}\".format(str(url.page_source))\n",
        "  currenturl = urlabout[i]\n",
        "  # get response object\n",
        "  try: \n",
        "    response_about = requests.get(currenturl)\n",
        "  except requests.exceptions.ConnectionError:\n",
        "    r.status_code = \"Connection refused\"\n",
        "    \n",
        "  \n",
        "  # get byte string \n",
        "  byte_data_about = response_about.content\n",
        "  # get filtered source code \n",
        "  source_code_about = html.fromstring(byte_data_about)\n",
        "  for ib in range(10):\n",
        "\n",
        "    name = \"EMPTY\"\n",
        "    domain = \"EMPTY\"\n",
        "    phone = \"EMPTY\"\n",
        "    mapaddress = \"EMPTY\"\n",
        "    category = \"EMPTY\"\n",
        "    search_facebook_results = []\n",
        "    search_email_results = []\n",
        "    is_hyperlink = []\n",
        "    hyperlink_leftovers = []\n",
        "\n",
        "    #company name\n",
        "    sc = source_code_about.xpath(namesxpathlist[ib])\n",
        "    #print( \"namesxpathlist[ib]: \", namesxpathlist[ib])\n",
        "    #print(\"sc: \", sc)\n",
        "    if len(sc) > 0:\n",
        "      sc = sc[0].text_content()\n",
        "      #Company name:\n",
        "      name = sc\n",
        "      #print(name)\n",
        "      #Specific yiptel url for company name:\n",
        "      urlregex = \"/biz/\"+ name.replace(\" \",\"-\").replace(\"â€™\",\"\").lower().replace(\".\",\"-\").replace(\",\",\"\").replace(\"----\",\"-\").replace(\"---\",\"-\").replace(\"--\",\"-\").replace(\"--\",\"-\").replace(\"--\",\"-\").replace(\"&\",\"and\")\n",
        "      #print(\"urlregex 1: \", urlregex)\n",
        "      urlregex = re.findall(urlregex.lower()+'(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', url)\n",
        "      #print(\"urlregex 2: \",urlregex)\n",
        "      try:\n",
        "        fencerurl = \"https://www.yelp.com.sg\" +urlregex[0]\n",
        "      except Exception:\n",
        "        pass\n",
        "      #print(\"fencerurl \",fencerurl)\n",
        "      try: \n",
        "        response_about2 = requests.get(fencerurl)\n",
        "      except Exception:\n",
        "        pass\n",
        "      \n",
        "      #print(\"2 \",name)\n",
        "\n",
        "      #Get Domain, category and phone:\n",
        "      try:\n",
        "\n",
        "        byte_data_about2 = response_about2.content\n",
        "        source_code_about2 = html.fromstring(byte_data_about2)\n",
        "        domain =  source_code_about2.xpath(domainxpath)\n",
        "        phone = source_code_about2.xpath(phonexpath)\n",
        "        category = source_code_about2.xpath(categoryxpath)\n",
        "        if domain[0]== \"+\":\n",
        "          phone = domain\n",
        "      except Exception:\n",
        "        pass\n",
        "      \n",
        "      if len(domain) > 0:\n",
        "        domain = domain[0].text_content()\n",
        "      else:\n",
        "        domain = \" http://ERROR\"\n",
        "      if len(phone) > 0:\n",
        "        phone = phone[0].text_content()\n",
        "      else:\n",
        "        phone = \"NOT FOUND\"\n",
        "      if len(category) > 0:\n",
        "        category = category[0].text_content()\n",
        "      else:\n",
        "        category = \" http://ERROR\"\n",
        "\n",
        "      #print(\"3 \",name)\n",
        "\n",
        "      \n",
        "\n",
        "      #get address:           \n",
        "  \n",
        "      addressxpath = '//address'\n",
        "      try:\n",
        "        mapaddress = urlregex[0]\n",
        "        mapaddress = mapaddress.split('?')\n",
        "        mapaddress = mapaddress[0]\n",
        "        mapaddress = mapaddress.split('/biz/')\n",
        "        mapaddress = mapaddress[1]\n",
        "        mapaddress = 'https://www.yelp.com.sg/map/'+mapaddress\n",
        "      except Exception:\n",
        "        pass\n",
        "\n",
        "      try:\n",
        "        try: \n",
        "         response_about3 = requests.get(mapaddress)\n",
        "        except requests.exceptions.ConnectionError:\n",
        "\n",
        "          r.status_code = \"Connection refused\"\n",
        "      except Exception:\n",
        "        pass             \n",
        "\n",
        "      #print(\"4 \",name)\n",
        "\n",
        "\n",
        "      try: \n",
        "        byte_data_about3 = response_about3.content\n",
        "        source_code_about3 = html.fromstring(byte_data_about3)\n",
        "              \n",
        "        mapaddress =  source_code_about3.xpath(addressxpath)\n",
        "        if len(mapaddress) >0:\n",
        "          mapaddress = mapaddress[0].text_content()\n",
        "          #mapaddress = mapaddress.split('Ste D')\n",
        "          #mapaddress = mapaddress[0] + \" , \" + mapaddress[1]\n",
        "          #mapaddress = mapaddress.split('United States')\n",
        "          #mapaddress = mapaddress[0] + \", USA\"\n",
        "        else: \n",
        "          mapaddress = \"NOT FOUND!\"\n",
        "      except Exception:\n",
        "        pass\n",
        "\n",
        "      time.sleep(random.randint(5,14))\n",
        "\n",
        "      #get email and facebook:\n",
        "\n",
        "      search_email_list = [\"https://www.\"+ domain, \n",
        "                          \"https://www.\"+ domain + \"/contact\", \n",
        "                          \"https://www.\"+ domain + \"/contact-us\",\n",
        "                          \"https://www.\"+ domain + \"/about\" , \n",
        "                          \"https://www.\"+ domain + \"/about-us\",\n",
        "\n",
        "\n",
        "\n",
        "                          \n",
        "                          \n",
        "                          \n",
        "                          ]\n",
        "      domain_email = domain\n",
        "      search_facebook_results = [] \n",
        "      search_email_results = []\n",
        "      search_email_results_unique = []\n",
        "\n",
        "      #search for Facebook account:\n",
        "      try:\n",
        "        for facebooksearch in range(len(search_email_list)):\n",
        "          \n",
        "          wb = webdriver.Chrome(\"chromedriver\",options=options)\n",
        "          is_hyperlink = search_email_list[facebooksearch]\n",
        "\n",
        "          \n",
        "          if len(is_hyperlink) > 5:\n",
        "            domain_check = is_hyperlink[-4]+is_hyperlink[-3]+is_hyperlink[-2]+is_hyperlink[-1]\n",
        "            if domain_check == '.com' or domain_check == '.net' or domain_check == '.org' or domain_check == '.gov' or domain_check == 'tact' or domain_check == 't-us' or domain_check == 'bout':\n",
        "              try:\n",
        "                wb.get(search_email_list[facebooksearch])\n",
        "                wb = r\"{}\".format(str(wb.page_source))\n",
        "                facebook_found = re.findall(r'https://www.facebook.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', wb)\n",
        "                search_facebook_results = search_facebook_results+ facebook_found\n",
        "              except Exception:\n",
        "                pass\n",
        "      except Exception:\n",
        "        pass\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "      #search for email in domain name\n",
        "\n",
        "      for emailsearch in range(len(search_email_list)):\n",
        "        #print(search_email_list[emailsearch])\n",
        "        wb = webdriver.Chrome(\"chromedriver\",options=options)\n",
        "        is_hyperlink = search_email_list[emailsearch]\n",
        "        if len(search_email_list[emailsearch]) > 5:\n",
        "          domain_check = is_hyperlink[-4]+is_hyperlink[-3]+is_hyperlink[-2]+is_hyperlink[-1]\n",
        "          if domain_check == '.com' or domain_check == '.net' or domain_check == '.org' or domain_check == '.gov'or domain_check == 'tact' or domain_check == 't-us' or domain_check == 'bout':\n",
        "            try:     \n",
        "              wb.get(search_email_list[emailsearch])\n",
        "              wb = r\"{}\".format(str(wb.page_source))\n",
        "              email_found = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', wb)\n",
        "              search_email_results = search_email_results + email_found\n",
        "            except Exception:\n",
        "              pass\n",
        "\n",
        "            try:     \n",
        "              wb.get(search_email_list[emailsearch])\n",
        "              wb = r\"{}\".format(str(wb.page_source))\n",
        "              email_found = re.findall(r'[\\w\\.-]+ @ [\\w\\.-]+', wb)\n",
        "              search_email_results = search_email_results + email_found\n",
        "            except Exception:\n",
        "              pass\n",
        "\n",
        "                \n",
        "\n",
        "        #search for email in Facebook account:\n",
        "\n",
        "      try: \n",
        "        for emailsearch in range(len(search_email_list)):\n",
        "          #print(search_email_list[emailsearch])\n",
        "          wb = webdriver.Chrome(\"chromedriver\",options=options)\n",
        "          is_hyperlink = search_email_list[emailsearch]\n",
        "          if len(search_email_list[emailsearch]) > 0:\n",
        "            domain_check = is_hyperlink[emailsearch][-4]+is_hyperlink[emailsearch][-3]+is_hyperlink[emailsearch][-2]+is_hyperlink[emailsearch][-1]\n",
        "            if domain_check == '.com' or domain_check == '.net' or domain_check == '.org' or domain_check == '.gov':     \n",
        "              wb.get(search_email_list[emailsearch])\n",
        "              wb = r\"{}\".format(str(wb.page_source))\n",
        "              email_found = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', wb)\n",
        "              search_email_results = search_email_results + email_found\n",
        "\n",
        "      except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "#old facebook search\n",
        "\n",
        "      # if len(search_facebook_results) >0:\n",
        "      #   for emailsearch in range(len(search_facebook_results)):\n",
        "      #     #print(search_email_list[emailsearch])\n",
        "      #     wb = webdriver.Chrome(\"chromedriver\",options=options)\n",
        "      #     wb.get(search_facebook_results[emailsearch]+'about/')\n",
        "      #     wb = r\"{}\".format(str(wb.page_source))\n",
        "      #     email_found = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', wb)\n",
        "      #     search_email_results = search_email_results + email_found \n",
        "\n",
        "\n",
        "\n",
        "#hyperlink check\n",
        "      is_hyperlink = search_email_results\n",
        "      for emails in range(len(is_hyperlink)):\n",
        "        if len(search_email_results[emails]) >5:\n",
        "          domain_check = is_hyperlink[emails][-4]+is_hyperlink[emails][-3]+is_hyperlink[emails][-2]+is_hyperlink[emails][-1]\n",
        "          if domain_check == '.com' or domain_check =='.net' or domain_check == '.org' or domain_check == '.gov':\n",
        "            hyperlink_leftovers.append(search_email_results[emails])\n",
        "        \n",
        "      search_email_results = hyperlink_leftovers    \n",
        "      search_email_results = pd.unique(search_email_results)\n",
        "      search_facebook_results = pd.unique(search_facebook_results)\n",
        "\n",
        "\n",
        "\n",
        "      #print(search_email_results)\n",
        "      \n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "#End of If Statement if Name not Found:\n",
        "    else:\n",
        "      name = \"ERROR!\"\n",
        "      fencerurl = \" http://ERROR\"\n",
        "      \n",
        "      \n",
        "    \n",
        "    mapaddress = mapaddress.replace(\"/n\",\"\").strip()\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    #Print out statement as list:\n",
        "    if domain[0]== \"+\":\n",
        "      rowlist = [name, \"DOMAIN NOT FOUND\", search_facebook_results, domain, category, mapaddress, search_email_results]\n",
        "      print(rowlist)\n",
        "\n",
        "    else:\n",
        "      rowlist = [ name, domain,search_facebook_results, phone, category, mapaddress, search_email_results]\n",
        "      print(rowlist)\n",
        "\n",
        "    to_df.append(rowlist)\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "    name = \"EMPTY\"\n",
        "    domain = \"EMPTY\"\n",
        "    phone = \"EMPTY\"\n",
        "    mapaddress = \"EMPTY\"\n",
        "    category = \"EMPTY\"\n",
        "    search_facebook_results = []\n",
        "    search_email_results = []\n",
        "    is_hyperlink = []\n",
        "    hyperlink_leftovers = []\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n FIN\\n\\n\")\n",
        "columns = [ \"Name\", \"URL\", \"Facebook\", \"Phone\", \"Category\", \"Address\", \"Emails\"]\n",
        "df = pd.DataFrame(to_df, columns = columns )\n",
        "df\n",
        "from google.colab import files\n",
        "\n",
        "df.to_csv(csv_name)\n",
        "files.download(csv_name)\n",
        "\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7uQwlBBISAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [ \"Name\", \"URL\", \"Facebook\", \"Phone\", \"Category\", \"Address\", \"Emails\"]\n",
        "\n",
        "\n",
        "df = pd.DataFrame(to_df, columns = columns )\n",
        "df\n",
        "from google.colab import files\n",
        "\n",
        "df.to_csv(csv_name)\n",
        "files.download(csv_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}